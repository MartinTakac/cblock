BL_constant_module
number_of_values=31

# manual control signals
value_names[0]=resetSeq_qml
values[0]=real 1 0.
value_names[1]=finalizeSeq_qml
values[1]=real 1 0.

# weight control
value_names[2]=reset_all_weights
values[2]=real 1 0.
value_names[3]=save_all_weights
values[3]=real 1 0.
value_names[4]=load_all_weights
values[4]=real 1 0.

# SOM inspection (qml widget coordinates)
value_names[5]=inspect_SOM_x
values[5]=real 1 0.
value_names[6]=inspect_SOM_y
values[6]=real 1 0.

# training control
value_names[7]=seq_learn_non_goal_driven
values[7]=real 1 1.
value_names[8]=seq_learn_goal_driven
values[8]=real 1 0.
value_names[9]=plan_learn_non_goal_driven
values[9]=real 1 1.
value_names[10]=plan_learn_goal_driven
values[10]=real 1 0.

# sequencing related parameters
# how much of a plan winner should be mixed with the predicted output at the start of a plan
value_names[11]=kick_start_gain
values[11]=real 1 0.
# influence from plan_som on seq_som's tonic 
value_names[12]=plan_top_down_on_seq_gain_non_goal_driven
values[12]=real 1 0.99
value_names[13]=plan_top_down_on_seq_gain_goal_driven
values[13]=real 1 1.

# multiplicative thr on avg (surp if error>thr*avg)
value_names[14]=surprise_avg_mult
values[14]=real 1 2.

# decay on tonic
value_names[15]=tonic_decay
values[15]=real 1 0.6
# decay on context
value_names[16]=prev_context_mix
values[16]=real 1 0.6
# mixing coef for amnestic error avg
value_names[17]=prev_avg_err_mix
values[17]=real 1 0.9
# truncate surp value (to avoid contaminating avg with infinity)
value_names[18]=surp_trunc_value
values[18]=real 1 5
# smoothing added to each element of predicted distribution to avoid infinite KL divergence at zeros.
value_names[19]=smoothing
values[19]=real #INDIV_SOM_SIZE# 0.0000000000001
# if entropy<entropy_thr, prediction is good enough
value_names[20]=entropy_thr
values[20]=real 1 0.35

# what type of input is cblock connected to (x,y or bitmap)
value_names[21]=indiv_xy
values[21]=real 1 0.
# if true, surprise is based on mismatch in winner, otherwise KL and sliding avg based.
value_names[22]=winner_match_surprise
values[22]=real 1 0.

# if high, cblock will internally compute and store differences (deltas) between the current state and goal state (or initial and final state of a chunk)
value_names[23]=use_delta_based_plans
values[23]=real 1 1.

# if dist_from_goal/winner_activity_raw>goal_reached_thr, the goal will be considered reached
value_names[24]=goal_reached_thr
values[24]=real 1 0.9

# plan-som/winner_activity_raw>plan_goodness_thr, plan is considered good
value_names[25]=plan_goodness_thr
values[25]=real 1 0.1

# index of buffer contents to display (if -1, the last elem is displayed)
value_names[26]=inspect_buf_index
values[26]=real 1 -1.

# some som-related displays are only updated next time som's perform_time_step=1. Turn these on briefly if you want to make sure you see the actual state (e.g. when fiddling with qmls). Then turn back off, as soms take lots of fps
value_names[27]=display_plan_som_real_time
values[27]=real 1 0.

value_names[28]=display_seq_som_real_time
values[28]=real 1 0.

# keep seq_som's surprise alphas till resetSeq or finalizeSeq in goal-driven and non-goal-driven 
value_names[29]=surprise_alphas_till_eos_gd
values[29]=real 1 0.

value_names[30]=surprise_alphas_till_eos_ngd
values[30]=real 1 1.
